h1. Enhanced Search Uses Case

h2. Introduction

This uses case aims to demonstrate a combination of MarkLogic's built-in _full-text_ XQuery XML content search and SPARQL _semantic_ RDF search. The content domain is BBC World News news items.

News Items, news stories are collected from the BBC News (UK edition) RSS feed for the 'World News' category: [http://feeds.bbci.co.uk/news/world/rss.xml?edition=uk], passed through HTML Tidy to convert them to well-formed, further filtered to extract only the essential content of the story and then term extraction is applied using Thomson Reuter's OpenCalais web service to discover the following entity/concept (subjects) types:
* Locations (Cities and Countries)
* Organisations (Businesses and Governmental organisations)
* People and their Roles
that are related to the story. The XHTML content is sent to OpenCalias via an HTTP request. A relevance score is generated by Open Calais for each entity it finds (this can be used later for filtering subjects).
OpenCalais returns these subjects described using RDF/XML which is added with the original XHTML to a _job-bag_ document and stored on disk prior to ingestion into MarkLogic.

The source code for this application is in the MarkLogic SVN repositiory at:

{{svn.marklogic.com/project/svnroot/ps/trunk/users/pfennell/use-cases/enhanced-search}}


h2. BBC World News Feed

The BBC World News news feed identifies news items via a unique URI identifier. Each News Item's web page can be retrieved using this URI.

THe HTML representation of the News Item contains embedded metadata using the rNews vocabulary created by the New York Times and IPTC. The relevant terms from this vocabulary that are found in the head of the web page are:
* rnews:creator
* rnews:datePublished
* rnews:description
* rnews:headline
* rnews:thumbnailUrl

In addition the following terms are extracted from the HTML:

* rnews:articleSection
* rnews:identifier
* rnews:usageTerms

For each image referenced in the body of the News Item a separate description is generated with the following properties (* where available):

* rnews:associatedArticle
* rnews:description
* rnews:encodingFormat
* rnews:height*
* rnews:width*

This information forms the base metadata for the news item as provided by the creator, the BBC.

The actual News Item story content is contained within an HTML DIV section that carries the 'story-body' CSS class. Embedded within this content can be sections that are otherwise extraneous to the actual story content and these are removed along with ans references to video content or scripts. Related links are kept.


h2. OpenCalais Entity/Concept Extraction

The 'story-body' of the News Item is serialised to a string and sent to the OpenCalais web service [http://www.opencalais.com/] with the following request configuration:

<oc:params xmlns:oc="http://s.opencalais.com/1/pred/">
  <oc:processingDirectives 
      oc:contentType="TEXT/HTML" 
      oc:outputFormat="xml/rdf" 
      oc:docRDFaccesible="false" 
      oc:allowDistribution="false"/>
  <oc:userDirectives 
      oc:allowDistribution="true" 
      oc:allowSearch="false" 
      oc:externalID="mluces" 
      oc:submitter="ml"/>
  <oc:externalMetadata/>
</oc:params>

Note: You will need an OpenCalais Service Key in order to use this service:

[http://www.opencalais.com/APIkey]

Without going into excessive detail, which is covered by the service documentation, OpenCalais will return the entities and concepts (subjects) it discovers within the text of the document as RDF, in this case RDF/XML and there are two sorts of information related to these subjects. Firstly it returns the information it has about them, for example a countries Prime Minister will have a description including their name, common name and their role (Prime Minister). Secondly it returns information on how and where that information was found, the relevance of the subject to the story (as a number =< 1) and the position within the text. The later part is there to allow you to embed the information as mark-up into the original text but we're not interested in that. All we keep is the relevance as that may be used to filter which subjects are returnd for a given News Item.

There are some complexities to the graph that is generated for each News Item's subjects. You can only really link subjects to the News Item via the relevance information. Descriptions that directly reference the {{c:docId}} property can be used to 'ground' a subject within the News Item.

Part of the Terms of Usgae of Open Calais requires you to main tain their subject URIs so, strictly speaking, mapping to a new vocabulary that's easier to query breaks this requirement.

The other assertion that is introduced is a link between the OpeCalais subject metadata and the BBC News Item metadata. We currently use an {{owl:sameAs}}


h2. News Item and Metadata Extraction Pipeline

The News Item extraction process has been prototyped using the XProc Pipeline Language in order that we can build a library of News Items that can by ingested independently of the original content and term extraction services. 

{{enhanced-search/src/main/resources/pipelines/enrich-news-pages.xpl}}

A usueful activitity would be to re-implement this process using MarkLogic's Content Processing Pipelines (CPF) or an equivalent XQuery processes, or poterntially using XProcXQ.

The content and metadata of the News Items are placed into a single document, a _job-bag_ who's very simple format is as follows:

<es:job-bag xmlns:es="http://www.marklogic.com/enhanced-search#">
  <html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:rnews="http://iptc.org/std/rNews/2011-10-07#"
      xml:lang="en-GB"
      xml:base="http://www.bbc.co.uk">
    ...
  </html>
  <rdf:RDF>
    ...
  </rdf:RDF>
</es:job-bag>


h2. Ingestion

As this work was originally started before ML7 EA3 became available the most obvious way to load the News Item content and metadata into MarkLogic was to use an Information Studio Flow which incorporated additional steps to extract the metadata graph and the story content from the job-bag. The two transformation steps used, in this order, are:

||#||Transform||Source||
|1|Extract Graph|{{enhanced-search/src/main/xquery/ingestion/extract-graph.xqy}}|
|2|Extract Content|{{enhanced-search/src/main/resources/transforms/extract-content.xsl}}|

The Flow sets a collection:

{{http://www.bbc.co.uk/news/content}}

for the news item content and the metadata is inserted into MarkLogic, via {{sem:rdf-insert()}} to the graph:

{{http://www.bbc.co.uk/news/graph}}

Alternatively, {{mlcp}} (1.1+) can be used to load both content and graph data as separate documents. There's another pipeline {{enhanced-search/src/main/resources/pipelines/open-job-bag.xpl}} that will split the graph and content data into separate {{.rdf}} and {{.xml}} files respectively.

The {{mlcp}} invocations used are as follows:

Graph:

import -host localhost -port 8022 -username admin -password admin -input_file_path graph -mode local -input_file_type RDF -output_collections "http://www.bbc.co.uk/news/graph" -output_uri_replace "Users/pfennell/Projects/MarkLogic/users/pfennell/use-cases/enhanced-search/data/ingest/graph,'graph/news'"

Content:

import -host localhost -port 8022 -username admin -password admin -input_file_path content -mode local -input_file_type documents -output_collections "http://www.bbc.co.uk/news/content" -output_uri_replace "Users/pfennell/Projects/MarkLogic/users/pfennell/use-cases/enhanced-search/data/ingest/content,'content/news'"


h2. Configuring Databases and Applications

A Configuration Manager package exists at:

{{enhanced-search/config/enhanced-search-package.zip}}

which provides configurations for the necessary databases and app-servers.


h2. Querying The News Graph

All the SPARQL queries that have been developed to-date, plus a few useful XQuery scripts, are included within a Query Console Workspace that can be found at:

{{enhanced-search/config/Enhanced_Search.xml}}

The SPARQL queries have been marked-up with {{sparql-doc}} comments:

[https://github.com/ldodds/sparql-doc]

Some of the queries include:

* All News Item headlines in descending date order.
* All Person's Position name in alphabetical order.
* All people who have a position name of 'President'.
* All cities sorted by containing country and name.
* OpenCalais categories for a News Item.
* All subjects for a given News Item.


h2. Things still to be done

If I had more time the the following things would be done.

# Introduce link into News Item entries in the search result feed that takes you to the selected News Item.
# Retrieving a News Item gives you access to the original story content and the related subject IRIs extracted by OpenCalais.
# Retrieving a subject will provide more information about the subject e.g. 'Barack Obama' has a position of 'President' and related country is 'United States'.
# As each subject is a _thing_ in its own right, therefore further _semantic_ searches can be made to find other News Items that are related to the context subject or related properties.
# An additional piece of work would be to use the DBpedia Look-up Service [http://wiki.dbpedia.org/lookup/] to link the News Item subjects to concepts in DBPedia.
# With the identified links between subjects and DBpedia concepts, import relevant sections of DBPedia to allow further dimensions of metadata and related links.
# Last, but by no means least, build a Web front-end that gives a human the chance to use this service in their browser.
 


